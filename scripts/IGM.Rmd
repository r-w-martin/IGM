---
title: "Integrated growth models for census and cmr fish data"
author: "Roy Martin"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  github_document:
    number_sections: TRUE
    df_print: "tibble"
    math_method: 
      engine: webtex
    #  url: https://latex.codecogs.com/svg.image?
    html_preview: TRUE
    keep_html: TRUE
bibliography: references.bib
link-citations: yes
---


```{r setup}
library(MASS) # for rmvnorm
library(tidyverse)
library(ggplot2)
library(ggExtra)
library(nimble) # for lkj functions
library(rstan)
library(loo)
library(bayesplot)
library(tidybayes)

MyNorm <- function(x) {
  (x - mean(x)) / (sd(x) * 2)
  } # center and scale

options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)

# Load model and data objects from drive for knitting (if they already exist)
## vector of object names
objs <- c("mix_fit_1",
          "cmr_fit_1")

N <- length(objs)

# list of file paths for objects saved
f <- list()
for(i in 1:N){
    f[i] <- paste0("./../modelFiles/",
    objs[i], ".rda")
    }

# load files if they exist
for(i in 1:N){
  if(file.exists(f[[i]]))
    load(f[[i]])
}

# clean up workspace
rm(f, i, objs, N)
```

# Simulate a mixture based on VB
Below we simulate a mixture of fish lengths based on a VB model for fish length at age.
```{r simulate_VB_mixture}
set.seed(1234)

N <- 1000 # number of individual fish
J <- 30 # number of hypothetical sampling sites fish belong to
A <- 5 # maximum age for fish in model population

theta <- nimble::rdirch(n =1, alpha = rev(seq(1:A)) * 500 ) # mixture probabilities: probability of belonging to length (age) class
class <- sample(1:A, size = N, prob = theta, replace = TRUE) # assign age class / draw which model each of 1,..,N fish  belongs to

site <- sample(1:J, size = N, replace = TRUE) # randomly assign fish to a site
y <- rep(NA, N) # container for fish lengths to be simulated

b0_L0 <- log(25) # log-scale intercept term for initial length mm / L0
b0_Linf <- log(250) # log-scale intercept term for asymptotic average length in mm / Linf  
b0_k <- log(0.4) # log-scale intercept term for Brody growth rate coefficient (growth in mm/time)

# assemble a hypothetical Cholesky correlation matrix for correlated VB parameters among sites
Omega <- matrix(NA, 3, 3) # container for correlations
Omega[1, ] <- c(1, -0.5, 0.5)
Omega[2, ] <- c(-0.5, 1, -0.2)
Omega[3, ] <- c(0.5, -0.2, 1)

# assemble a covariance matrix with site-to-site correlations among parameters and scale of variation
sigma_VB <- c(0.2, 0.2, 0.2) # scale of site-to-site variation in L0, Linf, and k, respectively
Sigma_VB <- as.matrix(diag(sigma_VB) %*% Omega %*% diag(sigma_VB)) # quad_form_diag() in Stan

# define random effects by site for each VB parameter
eps <- matrix(NA, J, 3) # container for effects

for(j in 1:J){
  eps[j, ] <- MASS::mvrnorm(1, mu = rep(0, 3), Sigma = Sigma_VB)
}

# assemble linear predictor for each VB parameter
L0 <- rep(NA, J)
Linf <- rep(NA, J)
k <- rep(NA, J)

for(j in 1:J){
  L0[j] <- exp(b0_L0 + eps[j, 1])
  Linf[j] <- exp(b0_Linf + eps[j, 2])
  k[j] <- exp(b0_k + eps[j, 3])
}

mu <- matrix(NA, J, A) # container for location parameter/mean of length at age

for(j in 1:J){
  mu[j, 1] <- L0[j] # mean length of age class 1
  for(a in 2:A){
    mu[j, a] <- L0[j] + (Linf[j] - L0[j]) * (1 - exp(-k[j] * (a - 1)))
    }
  }

log_mu <- log(mu) # log mu for location parameter of lognormal likelihood

sigma <- 0.15 # scale of observation-level variation in lengths

# lognormal likelihood
for(n in 1:N){
  y[n] <- rlnorm(1, log_mu[site[n], class[n]], sigma)
}
```

Plot the simulated length-frequency
```{r plot_simulated_lengths, fig.align='center', fig.asp=0.75, fig.width=5}
y %>%
  data.frame(length = y) %>%
  ggplot(aes(x = length)) + 
  geom_histogram(aes(y = after_stat(count / sum(count))), binwidth = 5) +
  ggtitle("Simulated length-frequency") +
  ylab("Percent of total unique lengths") +
  xlab("Length (mm)")
```

# Mixture model for VB growth from lengths
Next we try to fit a model to the simulated data and recover the original fixed values for $\theta$ and the $\beta$ parameters.
```{stan model_1, echo=TRUE, message=FALSE, warning=FALSE, output.var="mix"}
data {
 int <lower = 1> A; // number of mixture components (ages)
 int <lower = 1> N; // n of obs (fish lengths - census data) data)
 int <lower= 1> J; // number of sites
 int <lower = 1> site[N];
 vector[N] y;
 real <lower = 0> eta; // parameter for LKJ prior
 vector <lower = 0> [A] alpha; // parameter for dirichlet prior
 int <lower =0, upper = 1> prior_only; // =1 to get data from prior predictive
 }
 
parameters {
 simplex[A] theta [J]; // mixture proportions
 ordered [2] b0_L; // b0 for L0[1] and Linf[2]
 real b0_k;
 real <lower = 0> sigma;
 vector <lower = 0> [3] sigma_VB;
 matrix[3 , J] r_z;
 cholesky_factor_corr[3] L_Omega;
}

transformed parameters {
 matrix[J, 3] eps;
 //ordered[A] mu [J]; 
 matrix [J, A] mu;
 vector [J] Linf;
 vector [J] k;
 vector [J] L0;

// random effects Linf, k, and t0
 eps = (diag_pre_multiply(sigma_VB, L_Omega) * r_z)'; 

// linear predictors for sub-models of VB parameters 
 L0 = exp(b0_L[1] + col(eps , 1));
 Linf = exp(b0_L[2] + col(eps , 2));
 k = exp(b0_k + col(eps , 3));
 
 //for (j in 1:J) {
   mu[, 1] = log(L0);
   for (a in 2:A) {
     mu[, a] = log(L0 + (Linf - L0) .* (1 - exp(-k .* (a - 1))));
     }
   //}
 }
 
model {
 //priors
 target += normal_lpdf(b0_L[1] | 3, 0.5); // b0_L0
 target += normal_lpdf(b0_L[2] | 5.5, 0.25); // b0_Linf
 target += normal_lpdf(b0_k | -1, 0.5);
 
 target += normal_lpdf(sigma_VB[1] | 0, 0.25); // Linf
 target += normal_lpdf(sigma_VB[2] | 0, 0.25); // k
 target += normal_lpdf(sigma_VB[3] | 0, 0.25); // L0
 
 target += normal_lpdf(to_vector(r_z) | 0, 1);
 
 target += normal_lpdf(sigma | 0, 0.5);
 
 target += lkj_corr_cholesky_lpdf(L_Omega | eta);
 
 for(j in 1:J) {
  target += dirichlet_lpdf(theta[j] | alpha);
  }
  
 //likelihood
 {
  vector[A] log_theta [J] = log(theta);
  vector[A] lps [J];
 
  for (n in 1:N) {
    for (a in 1:A) { 
      lps[site[n], a] = log_theta[site[n], a] + lognormal_lpdf(y[n] | mu[site[n], a] , sigma);
      }
      if (prior_only == 0){
        target += log_sum_exp(lps[site[n]]);
        }
    }
  }
 }
 
generated quantities {
 // Generate a component identifier "comp"
 // then draw "y_rep" using correct components
 // for mu and sigma
 int <lower = 1, upper = A> comp[N];
 vector[N] y_rep;
 vector[N] log_lik;
 matrix[3 , 3] Omega;
 
 Omega = multiply_lower_tri_self_transpose(L_Omega);
 {
 vector[A] log_theta [J] = log(theta); 
 vector[A] lps [J];

  for (n in 1:N) {
   for (a in 1:A) {
     lps[site[n], a] = log_theta[site[n], a] + 
      lognormal_lpdf(y[n] | mu[site[n], a] , sigma);
     }
    log_lik[n] = log_sum_exp(lps[site[n]]);
    comp[n] = categorical_rng(theta[site[n]]);
    y_rep[n] = lognormal_rng(mu[site[n], comp[n]] , sigma);
   } 
  }
 }
```

## Fit model to simulated data
Lets now fit our model to the observational data, again looking at 5 age classes.

### Data list for fit to data
```{r stan_data_list_fit_1}
stan_dataList_mix_fit <- list(N = N,
                              J = J,
                              site = site,
                              y = y,
                              A = A, 
                              eta = 1,
                              alpha = rep(1, 5),
                              prior_only = 0
                              )
```

### Fit the model to simulated data
```{r fit_stan_model_1, eval=FALSE, include=TRUE}
# takes about 692s
mix_fit_1 <- sampling(object = mix,
                      data = stan_dataList_mix_fit,
                      chains = 4,
                      iter = 3000,
                      cores = 4,
                      thin = 1,
                      seed = 234#,
                      #control = list(
                      #  adapt_delta=0.90, #default=0.8
                      #  max_treedepth =12 #default= 10
                      #  )
                      )

save(mix_fit_1, file = "./../modelFiles/mix_fit_1.rda")
```

### pairs plot of the posteriors
```{r pairs_summary_fit_1, fig.align='center', fig.asp=1, fig.width=8}
np <- nuts_params(mix_fit_1)

mcmc_pairs(mix_fit_1,
           pars = c("b0_L[1]",
                    "b0_L[2]",
                    "b0_k",
                    "lp__"),
           regex_pars = "sigma",
           np = np,
           off_diag_args = list(size = 0.75)
           )
```

### Tabular parameter summary
```{r print_summary_1, echo=TRUE}
print(mix_fit_1, 
      pars=c("b0_L",
             "b0_k",
             "sigma_VB",
             "sigma",
             "Omega",
             "lp__")
      )
```

### Estimates vs. true values
Did we recover the parameters of the simulation? True values are the red vertical lines.
```{r extract_posterior}
la_mix_1 <- extract(mix_fit_1)
```

```{r posterior_vs_true_betas, echo=TRUE, fig.align='center', fig.asp = 0.5, fig.width=6}
p <- la_mix_1$b0_L %>% data.frame() %>%
  rename(b_L0 = X1, 
         b_Linf = X2) %>%
  mutate(b_k = la_mix_1$b0_k) %>%
  pivot_longer(cols = c(b_L0, b_Linf, b_k),
               names_to = "parameter",
               values_to = "value") %>%
  ggplot(aes(x = value)) +
  geom_histogram(aes(y = after_stat(count / sum(count))), binwidth = 0.01) +
  facet_wrap(~ parameter, scales = "free_x") +
  ylab("Percent of total posterior draws")

tv <- data.frame(parameter = c("b_L0", "b_Linf", "b_k"), tv = c(b0_L0, b0_Linf,  b0_k))
p + geom_vline(aes(xintercept = tv), tv, color = "red", linewidth = 2)
```

```{r posterior_vs_true_sigma, echo=TRUE, fig.align='center', fig.asp = 0.5, fig.width=6}
p <- la_mix_1$sigma_VB %>% data.frame() %>%
  rename(sigma_L0 = X1, 
         sigma_Linf = X2,
         sigma_k = X3) %>%
  mutate(sigma_y = la_mix_1$sigma) %>%
  pivot_longer(cols = c(sigma_L0, sigma_Linf, sigma_k, sigma_y),
               names_to = "parameter",
               values_to = "value") %>%
  ggplot(aes(x = value)) +
  geom_histogram(aes(y = after_stat(count / sum(count))), binwidth = 0.0025) +
  facet_wrap(~ parameter, scales = "free_x", ncol = 4) +
  ylab("Percent of total posterior draws")

tv <- data.frame(parameter = c("sigma_L0", "sigma_Linf", "sigma_k", "sigma_y"), tv = c(sigma_VB, sigma))
p + geom_vline(aes(xintercept = tv), tv, color = "red", linewidth = 2)
```

```{r posterior_vs_true_omega, echo=TRUE, fig.align='center', fig.asp = 0.5, fig.width=6}
p <- la_mix_1$Omega[, 1, 2] %>% data.frame() %>%
  rename(Omega_b0_bLinf = ".") %>%
  mutate(Omega_b0_bk = la_mix_1$Omega[, 1, 3],
         Omega_bLinf_bk = la_mix_1$Omega[, 2, 3]) %>%
  pivot_longer(cols = c(Omega_b0_bLinf, Omega_b0_bk, Omega_bLinf_bk),
               names_to = "parameter",
               values_to = "value") %>%
  ggplot(aes(x = value)) +
  geom_histogram(aes(y = after_stat(count / sum(count))), binwidth = 0.05) +
  facet_wrap(~ parameter, scales = "free_x", ncol = 4) +
  ylab("Percent of total posterior draws")

tv <- data.frame(parameter = c("Omega_b0_bLinf", "Omega_b0_bk", "Omega_bLinf_bk"), tv = c(Omega[1,2], Omega[1,3], Omega[2,3]))
p + geom_vline(aes(xintercept = tv), tv, color = "red", linewidth = 2)
```

### Compare predictions to characteristics of simulated data
Compare posterior predictive distribution (black) to observed (simulated in this case) data distribution (red): 
```{r ppd_summary_1, echo=TRUE, fig.align='center', fig.asp = 0.6, fig.width=5}
bayesplot::ppc_stat(y = y, 
                    yrep = la_mix_1$y_rep, 
                    stat = mean,
                    binwidth = 1)

bayesplot::ppc_stat(y = y, 
                    yrep = la_mix_1$y_rep, 
                    stat = sd,
                    binwidth = 1)

bayesplot::ppc_stat(y = y, 
                    yrep = la_mix_1$y_rep, 
                    stat = max,
                    binwidth = 1)

bayesplot::ppc_stat(y = y, 
                    yrep = la_mix_1$y_rep, 
                    stat = min,
                    binwidth = 1)

s <- as.vector(sample(1:length(y), 100, replace = F))#draw random 100 rows

#bayesplot::ppc_dens_overlay( y = df_mixture$length , yrep = la_prior$y_rep_mix[ s , ] )

bayesplot::ppc_intervals(y = y[s], yrep = la_mix_1$y_rep[, s])

rm(s)

plot(density(la_mix_1$y_rep[1, ]), lwd = 2, main = "", ylim=c(0, 0.015), xlim=c(0, 500), col = 'gray')
for (i in 1:200){
  lines(density(la_mix_1$y_rep[sample(1:dim(la_mix_1$y_rep)[1], 1), ]), lwd=2, col = 'gray')
  }
lines(density(y), col = 'red', lwd = 2)
```

### Leave-one-out cross validation with PSIS-LOO
```{r loo_mix, fig.align='center', fig.height=4, fig.width=6 }

log_lik_mix_1 <- loo::extract_log_lik(mix_fit_1 , 
                                  parameter_name="log_lik", 
                                  merge_chains = FALSE)

r_eff_mix_1 <- loo::relative_eff(log_lik_mix_1 , cores = 1)

loo_mix_1 <- loo::loo(log_lik_mix_1, 
                  r_eff= r_eff_mix_1,
                  save_psis = TRUE,
                  cores = 1)
print(loo_mix_1)

plot(loo_mix_1 , label_points = T)
```

#### LOO-PIT calibration
Now lets use the loo calculations to graphically assess calibration. 

First, we'll need to extract the weights from the LOO PSIS object.
```{r extract_mix_1}
wts_mix_1 <- weights(loo_mix_1$psis_object)
```

Now we can plot the LOO-PIT overlay. If well-calibrated, the PIT line should fall within the simulated draws of the uniform distribution (blue lines).
```{r loo_pit_model_2, warning=FALSE, fig.align='center', fig.width=6, fig.asp=0.7}
bayesplot::ppc_loo_pit_overlay(y = stan_dataList_mix_fit$y, 
                    yrep = la_mix_1$y_rep,
                    lw = wts_mix_1,
                    samples = 50)
```

# Stan model for CMR data
Now lets consider the CMR data only.
```{stan model_2, echo=TRUE, message=FALSE, warning=FALSE, output.var="cmr"}
data{
 int< lower = 1 > N_cmr; // number of observations (fish lengths - cmr data)
 int< lower= 1 > J; // number of sites
 int < lower=1 > site_cmr[ N_cmr ];
 vector[ N_cmr ] y_cmr;
 vector[ N_cmr ] L_i;
 vector< lower = 0 >[ N_cmr ] days;
 real < lower=1 > eta; // parameter for LKJ prior
 int< lower=0 , upper=1 > prior_only; // =1 to get data from prior predictive
 }
parameters{
 real z_b0_Linf;
 real z_b0_k;
 matrix[ 2 , J ] r_z;
 vector < lower=0 > [ 2 ] z_sigma_VBG;
 cholesky_factor_corr[ 2 ] L_Omega_J;
 real < lower=0 > z_sigma_cmr;
 }
transformed parameters{
 real b0_Linf;
 real b0_k;
 matrix[ J , 2 ] r;
 vector [ J ] Linf;
 vector [ J ] k;
 vector < lower=0 > [ 2 ] sigma_VBG;
 real < lower=0 > sigma_cmr;
 vector [ N_cmr ] mu_cmr;
 
 b0_Linf = 5.35 + z_b0_Linf * 0.2; //translated: N( 250 , 30 ) #5.4, 0.25
 b0_k = -0.5 + z_b0_k * 1; //translated: N( 0 , 0.3 ) #-0.7, 0.5
 
 sigma_VBG[ 1 ] = z_sigma_VBG[ 1 ] * 0.3; // translated: hN( 0 , 15 ) Linf
 sigma_VBG[ 2 ] = z_sigma_VBG[ 2 ] * 0.5; // translated: N( 0 , 0.2 ) k
 
 sigma_cmr = z_sigma_cmr * 0.25; //translated: hN( 0 , 0.2 )
 
 r = ( diag_pre_multiply( sigma_VBG , L_Omega_J ) * r_z )';
 
 Linf = exp( b0_Linf + col( r , 1 ) );
 k = exp( b0_k + col( r , 2 ) );

 for ( c in 1:N_cmr ) {
  mu_cmr[ c ] =  log(
   L_i[ c ] + ( Linf[ site_cmr[ c ] ] - L_i[ c ] ) .* 
    ( 1.0 - exp( -k[ site_cmr[ c ] ] .* ( days[ c ] / 365 ) ) ) );
  }
 }
model{
 //priors
 target += normal_lpdf( z_b0_Linf | 0 , 1 );
 target += normal_lpdf( z_b0_k | 0 , 1 );
 
 target += normal_lpdf( to_vector( r_z ) | 0 , 1 );
 
 target += normal_lpdf( z_sigma_VBG | 0 , 1 ) - 1 * normal_lccdf( 0 | 0 , 1 );
 
 target += normal_lpdf( z_sigma_cmr | 0 , 1 ) - 1 * normal_lccdf( 0 | 0 , 1 );
 
 target += lkj_corr_cholesky_lpdf( L_Omega_J | eta );
 
 //likelihood
 if ( prior_only == 0 ) {
  target += lognormal_lpdf( y_cmr | mu_cmr, sigma_cmr );
  }
 }
generated quantities {
  vector[ N_cmr ] y_rep_cmr;
  vector[ N_cmr ] log_lik;
  matrix[ 2 , 2 ] Omega_J;
  
  Omega_J = multiply_lower_tri_self_transpose( L_Omega_J );
  
  for ( c in 1:N_cmr ) {
   log_lik[ c ] = lognormal_lpdf( y_cmr[ c ] | mu_cmr[ c ] , sigma_cmr );
   y_rep_cmr[ c ] = lognormal_rng( mu_cmr[ c ] , sigma_cmr );
   }
 }
```

## Fit model to observed data
Lets now fit our model to the observational data, again looking at 5 age classes.

### Data list for fit to data
```{r stan_data_list_fit_2}
stan_dataList_cmr_fit <- list(N_cmr=nrow(df_cmr),
                              J=max(coerce_index(df_cmr$site_year)),
                              site_cmr=coerce_index(df_cmr$site_year),
                              y_cmr=df_cmr$length_r,
                              L_i=df_cmr$length_c,
                              days = df_cmr$days,
                              eta = 2, 
                              prior_only = 0
                              )

```

### Run the model with observational data
```{r fit_stan_model_2, echo=TRUE, cache=TRUE}
cmr_fit_1 <- sampling(object=cmr,
                        data=stan_dataList_cmr_fit,
                        chains=5,
                        iter=2000,
                        cores=5,
                        thin=1#,
                        #control = list(
                        #  adapt_delta=0.99, #default=0.8
                        #  max_treedepth =12 #default= 10
                        #  )
                        )
```

### Pairs plot of the posteriors:
```{r pairs_summary_fit_2, echo=FALSE, fig.align='center', fig.width=8, fig.height=8}
pairs(cmr_fit_1, 
      pars=c("b0_Linf",
             "b0_k",
             "sigma_VBG",
             "sigma_cmr",
             "lp__"),
      log=TRUE
      )
```

### Summarize the parameters
```{r print_summary_2, echo=TRUE}
print(cmr_fit_1, 
      pars=c("b0_Linf",
             "b0_k",
             "sigma_VBG",
             "sigma_cmr",
             "Omega_J",
             "lp__")
      )
```

### Compare prior model to observational data
Compare prior predictive distribution (black) to observational data distribution (red): 
```{r ppd_summary_2, echo=TRUE, fig.align='center', fig.height=4, fig.width=6, message=FALSE, warning=FALSE}

la_fit_1 <- rstan::extract( cmr_fit_1 ) 

bayesplot::ppc_stat( y = df_cmr$length_r ,
                     yrep = la_fit_1$y_rep_cmr , 
                     stat = mean )

bayesplot::ppc_stat( y = df_cmr$length_r , 
                     yrep = la_fit_1$y_rep_cmr , 
                     stat = sd )

bayesplot::ppc_stat( y = df_cmr$length_r , 
                     yrep = la_fit_1$y_rep_cmr , 
                     stat = max )

bayesplot::ppc_stat( y = df_cmr$length_r , 
                     yrep = la_fit_1$y_rep_cmr , 
                     stat = min )

s <- as.vector(sample(1:nrow(df_cmr), 100, replace = F ))#draw random 100 rows

#bayesplot::ppc_dens_overlay( y = df_mixture$length , yrep = la_prior$y_rep_mix[ s , ] )

bayesplot::ppc_intervals( y = df_cmr$length_r[ s ], yrep = la_fit_1$y_rep_cmr[  , s ] )

rm(s)

plot( density( la_fit_1$y_rep_cmr[ 1 , ] ) , lwd=2 , main= "" , ylim=c( 0, 0.02 ), xlim=c( 0 , 500 ) , col='gray' )
for ( i in 1:200 ){
  lines( density( la_fit_1$y_rep_cmr[ sample( 1:dim(la_fit_1$y_rep_cmr)[1], 1 ) , ] ) , lwd=2 , col='gray' )
  }
lines( density ( df_cmr$length_r ) , col='red' , lwd=2 )
```

###Leave-one-out cross validation with PSIS-LOO
```{r loo_cmr, fig.align='center', fig.height=4, fig.width=6 }

log_lik_cmr_1 <- loo::extract_log_lik( cmr_fit_1 , 
                                  parameter_name="log_lik", 
                                  merge_chains = FALSE )

r_eff_cmr_1 <- loo::relative_eff( log_lik_cmr_1 , cores = 1 )

loo_cmr_1 <- loo::loo( log_lik_cmr_1, 
                  r_eff= r_eff_cmr_1 , 
                  cores = 1 )
print( loo_cmr_1 )

plot( loo_cmr_1 , label_points = T )

```

This diagnostic plot helps us identify potentially problematic observations, given the model. Lets print out a few of the potentially problematic observations.

```{r potential_bad}

print( df_cmr[ c( 64 , 355 , 589 ) , ] %>%
  mutate( grow_mm_d = round( ( length_r - length_c ) / days , 3 ) ) )

```

# Stan model for integrated (mixture and cmr) VBGF
Now lets integrate the two models (and data).
```{stan model_3, echo=TRUE, message=FALSE, warning=FALSE, output.var="igm"}
data{
 int< lower = 1 > A; // number of mixture components (ages)
 int< lower = 1 > N_mix; // n of obs (fish lengths - census data) data)
 int< lower = 1 > N_cmr; // number of observations (fish lengths - cmr data)
 int< lower= 1 > J; // number of sites
 int < lower=1 > site_mix[N_mix];
 int < lower=1 > site_cmr[N_cmr];
 vector[N_cmr] L_i;
 vector[N_cmr] days;
 vector[N_mix] y_mix;
 vector[N_cmr] y_cmr;
 real <lower=0> eta; // parameter for LKJ prior
 vector <lower=0> [A] alpha; // parameter for dirichlet prior
 int < lower=0 , upper=1 > prior_only; // =1 to get data from prior predictive
 }
parameters{
 simplex[ A ] theta [ J ]; // mixture proportions
 real z_b0_Linf;
 real z_b0_k;
 real z_b0_L0;
 matrix[ 3 , J ] r_z;
 vector < lower=0 > [ 3 ] z_sigma_VBG;
 cholesky_factor_corr[ 3 ] L_Omega_J;
 real < lower=0 > z_sigma_mix;
 real < lower=0 > z_sigma_cmr;
 }
transformed parameters{
 real b0_Linf;
 real b0_k;
 real b0_L0;
 matrix[ J , 3 ] r;
 vector [ J ] Linf;
 vector [ J ] k;
 vector [ J ] L0;
 vector < lower=0 > [ 3 ] sigma_VBG;
 real < lower=0 > sigma_mix;
 real < lower=0 > sigma_cmr;
 ordered[ A ] mu_mix [ J ];
 vector [ N_cmr ] mu_cmr;
 
 b0_Linf = 5.35 + z_b0_Linf * 0.2; //translated: N( 5.35 , 0.2 ) #5.4, 0.25
 b0_k = -0.5 + z_b0_k * 0.5; //translated: N( 0 , 0.5 ) #-0.7, 0.5
 b0_L0 = 3.5 + z_b0_L0 * 0.1; // translated: N( 3.5 , 0.1 ) #2.5, 0.5
 
 sigma_VBG[ 1 ] = z_sigma_VBG[ 1 ] * 0.3; // .3 translated: hN( 0 , 0.2 ) Linf
 sigma_VBG[ 2 ] = z_sigma_VBG[ 2 ] * 0.5; // .5 translated: N( 0 , 0.2 ) k
 sigma_VBG[ 3 ] = z_sigma_VBG[ 3 ] * 0.05; // translated: N( 0 , 0.05 ) L0
 
 sigma_mix = z_sigma_mix * 0.1; //translated: hN( 0 , 0.1 )
 sigma_cmr = z_sigma_cmr * 0.1; //translated: hN( 0 , 0.1 )
 
 r = ( diag_pre_multiply( sigma_VBG , L_Omega_J ) * r_z )'; // random effects Linf, k, and t0
 
 Linf = exp( b0_Linf + col( r , 1 ) );
 k = exp( b0_k + col( r , 2 ) );
 L0 = exp( b0_L0 + col( r , 3 ) );
 
 for ( j in 1:J ) {
  mu_mix[ j , 1 ] = log( L0[ j ] );
  for ( a in 2:A ) {
   mu_mix[ j , a ] = log( L0[ j ] + ( Linf[ j ] - L0[ j ] ) .* ( 1.0 - exp( -k[ j ] .* ( a - 1 )  ) ) );
   }
  }
  
 for ( c in 1:N_cmr ) {
  mu_cmr[ c ] =  log(
   L_i[ c ] + ( Linf[ site_cmr[ c ] ] - L_i[ c ] ) .* 
    ( 1.0 - exp( -k[ site_cmr[ c ] ] .* ( days[ c ] / 365 ) ) ) );
  }
 }
model{
 vector[ A ] log_theta [ J ];
 vector[ A ] lps [ J ];
 
 log_theta = log( theta );
 
 //priors
 target += normal_lpdf( z_b0_Linf | 0 , 1 );
 target += normal_lpdf( z_b0_k | 0 , 1 );
 target += normal_lpdf( z_b0_L0 | 0 , 1 );
 
 target += normal_lpdf( to_vector( r_z ) | 0 , 1 );
 
 target += normal_lpdf( z_sigma_VBG | 0 , 1 ) - 1 * normal_lccdf( 0 | 0 , 1 );
 
 target += normal_lpdf( z_sigma_mix | 0 , 1 ) - 1 * normal_lccdf( 0 | 0 , 1 );
 target += normal_lpdf( z_sigma_cmr | 0 , 1 ) - 1 * normal_lccdf( 0 | 0 , 1 );

 target += lkj_corr_cholesky_lpdf( L_Omega_J | eta );
 
 for( j in 1:J)
  target += dirichlet_lpdf( theta[ j ] | alpha );
 
 //likelihood
 for ( m in 1:N_mix ) {
  for ( a in 1:A ) { 
   lps[ site_mix[ m ] , a ] = log_theta[ site_mix[ m ] , a ] + 
     lognormal_lpdf( y_mix[ m ] |  mu_mix[ site_mix[ m ], a ] , sigma_mix );
   }
  if ( prior_only == 0 )
   target += log_sum_exp( lps[ site_mix[ m ] ] );
  }
 
 if ( prior_only == 0 ) {
  target += lognormal_lpdf( y_cmr | mu_cmr, sigma_cmr );
  }
 }
generated quantities {
 // for mixture we generate a component identifier "comp"
 // then draw "y_rep_mix" using correct component mu_mix and sigma
 // for cmr we draw "y_rep_cmr" using mu_cmr and sigma
 int< lower=1 , upper=A > comp[ N_mix ];
 vector[ A ] log_theta [ J ];
 vector[ N_mix ] y_rep_mix;
 vector[ N_mix ] log_lik_mix;
 vector[ N_cmr ] y_rep_cmr;
 vector[ N_cmr ] log_lik_cmr;
 vector[ N_mix + N_cmr ] log_lik;
 matrix[ 3 , 3 ] Omega_J;
 
 log_theta = log( theta );  
 Omega_J = multiply_lower_tri_self_transpose( L_Omega_J );
 
 for ( m in 1:N_mix ) {
  vector[ A ] lps [ J ];
  
  for ( a in 1:A ) {
   lps[ site_mix[ m ] , a ] = log_theta[ site_mix[ m ] , a ] + 
     lognormal_lpdf( y_mix[ m ] |  mu_mix[ site_mix[ m ] , a ] , sigma_mix );
   }
  
  log_lik_mix[ m ] = log_sum_exp( lps[ site_mix[ m ] ] ); 
  comp[ m ] = categorical_rng( theta[ site_mix[ m ] ] );
  y_rep_mix[ m ] = lognormal_rng( mu_mix[ site_mix[ m ], comp[ m ] ] , sigma_mix );
  }
 
 for ( c in 1:N_cmr ) {
  log_lik_cmr[ c ] = lognormal_lpdf( y_cmr[ c ] | mu_cmr[ c ] , sigma_cmr );
  y_rep_cmr[ c ] = lognormal_rng( mu_cmr[ c ] , sigma_cmr );
  }
 
 log_lik = append_row( log_lik_mix , log_lik_cmr );
 }
```

# Stan model for integrated (mixture and cmr) VBGF covariates
Now lets integrate the two models (and data) with covariates.
```{stan model_4, echo=TRUE, message=FALSE, warning=FALSE, output.var="igm"}
 data{
 int< lower = 1 > A; // number of mixture components (ages)
 int< lower = 1 > N_mix; // n of obs (fish lengths - census data) data)
 int< lower = 1 > N_cmr; // number of observations (fish lengths - cmr data)
 int< lower= 1 > J; // number of sites
 int < lower=1 > site_mix[ N_mix ];
 int < lower=1 > site_cmr[ N_cmr ];
 vector[ N_cmr ] L_i;
 vector[ N_cmr ] days;
 vector[ N_mix ] y_mix;
 vector[ N_cmr ] y_cmr;
 vector [ J ] temp;
 vector [ J ] effD;
 real < lower=1 > eta; // parameter for LKJ prior
 vector < lower=0 > [ A ] alpha; // parameter for dirichlet prior
 int < lower=0 , upper=1 > prior_only; // =1 to get data from prior predictive
}

parameters{
 simplex[ A ] theta [ J ]; // mixture proportions
 real z_b0_Linf;
 real z_b0_k;
 real z_b0_L0;
 real z_b1_k;
 real z_b1_Linf;
 real z_b2_k;
 real z_b2_Linf;
 real z_b3_k;
 real z_b3_Linf;
 matrix[ 3 , J ] r_z;
 vector < lower=0 > [ 3 ] z_sigma_VBG;
 cholesky_factor_corr[ 3 ] L_Omega_J;
 real < lower=0 > z_sigma_mix;
 real < lower=0 > z_sigma_cmr;
}

transformed parameters{
 real b0_Linf;
 real b0_k;
 real b0_L0;
 real b1_Linf;
 real b1_k;
 real b2_Linf;
 real b2_k;
 real b3_Linf;
 real b3_k;
 matrix[ J , 3 ] r;
 vector [ J ] L0;
 vector [ J ] Linf;
 vector [ J ] k;
 vector < lower=0 > [ 3 ] sigma_VBG;
 real < lower=0 > sigma_mix;
 real < lower=0 > sigma_cmr;
 ordered[ A ] mu_mix [ J ];
 vector [ N_cmr ] mu_cmr;

 b0_Linf = 5.35 + z_b0_Linf * 0.2; //translated: N( 5.35 , 0.2 )
 b0_k = -0.5 + z_b0_k * 0.5; //translated: N( -0.5 , 1 )
 b0_L0 = 3.5 + z_b0_L0 * 0.1; // translated: N( 3.5 , 0.1 ) #2.5, 0.5
 
 b1_Linf = z_b1_Linf * 0.5; //translated: N( 0 , 0.5 )
 b1_k = z_b1_k * 0.5; //translated: N( 0 , 0.5 )
 b2_Linf = z_b2_Linf * 0.5; //translated: N( 0 , 0.5 )
 b2_k = z_b2_k * 0.5; //translated: N( 0 , 0.5 )
 b3_Linf = z_b3_Linf * 0.3; //translated: N( 0 , 0.3 )
 b3_k = z_b3_k * 0.3; //translated: N( 0 , 0.3 )

 sigma_VBG[ 1 ] = z_sigma_VBG[ 1 ] * 0.3; // translated: hN( 0 , 0.3 ) Linf
 sigma_VBG[ 2 ] = z_sigma_VBG[ 2 ] * 0.5; // translated: N( 0 , 1 ) k
 sigma_VBG[ 3 ] = z_sigma_VBG[ 3 ] * 0.05; // translated: N( 0 , 0.05 ) L0

 sigma_mix = z_sigma_mix * 0.1; //translated: hN( 0 , 0.1 )
 sigma_cmr = z_sigma_cmr * 0.1; //translated: hN( 0 , 0.25 )

 r = ( diag_pre_multiply( sigma_VBG , L_Omega_J ) * r_z )';

 Linf = exp( b0_Linf + b1_Linf * temp + b2_Linf * effD + b3_Linf * ( effD .* temp ) + col( r , 1 ) );
 k = exp( b0_k + b1_k * temp + b2_k * effD + b3_k * ( effD .* temp ) + col( r , 2 ) );
 L0 = exp( b0_L0 + col( r , 3 ) );

 for ( j in 1:J ) {
  mu_mix[ j , 1 ] = log( L0[ j ] );
  for ( a in 2:A ) {
   mu_mix[ j , a ] = log( L0[ j ] + ( Linf[ j ] - L0[ j ] ) .* ( 1.0 - exp( -k[ j ] .* ( a - 1 )  ) ) );
   }
  }
  
for ( c in 1:N_cmr ) {
  mu_cmr[ c ] =  log( L_i[ c ] + ( Linf[ site_cmr[ c ] ] - L_i[ c ] ) .* 
                  ( 1.0 - exp( -k[ site_cmr[ c ] ] .* ( days[ c ] / 365 ) ) ) );
  }
}

model{
 vector[ A ] log_theta [ J ];
 vector[ A ] lps [ J ];
 
 log_theta = log( theta );

//priors
target += normal_lpdf( z_b0_Linf | 0 , 1 );
target += normal_lpdf( z_b0_k | 0 , 1 );
target += normal_lpdf( z_b1_Linf | 0 , 1 );
target += normal_lpdf( z_b1_k | 0 , 1 );
target += normal_lpdf( z_b2_Linf | 0 , 1 );
target += normal_lpdf( z_b2_k | 0 , 1 );
target += normal_lpdf( z_b3_Linf | 0 , 1 );
target += normal_lpdf( z_b3_k | 0 , 1 );

target += normal_lpdf( to_vector( r_z ) | 0 , 1 );

target += normal_lpdf( z_sigma_VBG | 0 , 1 ) - 1 * normal_lccdf( 0 | 0 , 1 );

target += normal_lpdf( z_sigma_cmr | 0 , 1 ) - 1 * normal_lccdf( 0 | 0 , 1 );

target += lkj_corr_cholesky_lpdf( L_Omega_J | eta );

 for( j in 1:J)
  target += dirichlet_lpdf( theta[ j ] | alpha );

//likelihood
 for ( m in 1:N_mix ) {
  for ( a in 1:A ) { 
   lps[ site_mix[ m ] , a ] = log_theta[ site_mix[ m ] , a ] + 
     lognormal_lpdf( y_mix[ m ] |  mu_mix[ site_mix[ m ], a ] , sigma_mix );
   }
  if ( prior_only == 0 )
   target += log_sum_exp( lps[ site_mix[ m ] ] );
  }
 
 if ( prior_only == 0 ) {
  target += lognormal_lpdf( y_cmr | mu_cmr, sigma_cmr );
  }
 }

generated quantities {
 // for mixture we generate a component identifier "comp"
 // then draw "y_rep_mix" using correct component mu_mix and sigma
 // for cmr we draw "y_rep_cmr" using mu_cmr and sigma
 int< lower=1 , upper=A > comp[ N_mix ];
 vector[ A ] log_theta [ J ];
 vector[ N_mix ] y_rep_mix;
 vector[ N_mix ] log_lik_mix;
 vector[ N_cmr ] y_rep_cmr;
 vector[ N_cmr ] log_lik_cmr;
 vector[ N_mix + N_cmr ] log_lik;
 matrix[ 3 , 3 ] Omega_J;
 
 log_theta = log( theta );  
 Omega_J = multiply_lower_tri_self_transpose( L_Omega_J );
 
 for ( m in 1:N_mix ) {
  vector[ A ] lps [ J ];
  
  for ( a in 1:A ) {
   lps[ site_mix[ m ] , a ] = log_theta[ site_mix[ m ] , a ] + 
     lognormal_lpdf( y_mix[ m ] |  mu_mix[ site_mix[ m ] , a ] , sigma_mix );
   }
  
  log_lik_mix[ m ] = log_sum_exp( lps[ site_mix[ m ] ] ); 
  comp[ m ] = categorical_rng( theta[ site_mix[ m ] ] );
  y_rep_mix[ m ] = lognormal_rng( mu_mix[ site_mix[ m ], comp[ m ] ] , sigma_mix );
  }
 
 for ( c in 1:N_cmr ) {
  log_lik_cmr[ c ] = lognormal_lpdf( y_cmr[ c ] | mu_cmr[ c ] , sigma_cmr );
  y_rep_cmr[ c ] = lognormal_rng( mu_cmr[ c ] , sigma_cmr );
  }
 
 log_lik = append_row( log_lik_mix , log_lik_cmr );
 }

```


# Fit model to observed data
Lets now fit our model to the observational data, again looking at 5 age classes.

## Data list for fit to data
```{r stan_data_list_fit_4}
stan_dataList_igm_fit <- list(N_mix = nrow(df_mixture),
                              N_cmr = nrow(df_cmr),
                              J = max(df_mixture$site_year),
                              site_mix = df_mixture$site_year,
                              site_cmr = df_cmr$site_year,
                              y_mix = df_mixture$length,
                              y_cmr = df_cmr$length_r,
                              L_i = df_cmr$length_c,
                              days = df_cmr$days,
                              effD = MyNorm(df_sites$effD),
                              temp = MyNorm(df_sites$temp),
                              eta = 2, 
                              A = 5,
                              alpha = rep( 2 , 5 ),
                              prior_only = 0
                              )
```

## Run the model with observational data
```{r fit_stan_model_4, echo=TRUE, cache=TRUE}
igm_fit_1 <- sampling(object=igm,
                        data=stan_dataList_igm_fit,
                        chains=5,
                        iter=2000,
                        cores=5,
                        thin=1#,
                        #control = list(
                        #  adapt_delta=0.99, #default=0.8
                        #  max_treedepth =12 #default= 10
                        #  )
                        )
```

## Pairs plot of the posteriors:
```{r pairs_summary_fit_3, echo=FALSE, fig.align='center', fig.width=8, fig.height=8}
pairs(igm_fit_1, 
      pars=c("b0_Linf",
             "b1_Linf",
             "b2_Linf",
             "b3_Linf",
             "b0_k",
             "b1_k",
             "b2_k",
             "b3_k",
             "sigma_VBG",
             "sigma_cmr",
             "lp__"),
      log=TRUE
      )
```

### Summarize the parameters
```{r print_summary_3, echo=TRUE}
print(igm_fit_1, 
      pars=c("b0_Linf",
             "b1_Linf",
             "b2_Linf",
             "b3_Linf",
             "b0_k",
             "b1_k",
             "b2_k",
             "b3_k",
             "sigma_VBG",
             "sigma_cmr",
             "Omega_J",
             "lp__")
      )
```

### Compare prior model to observational data
Compare prior predictive distribution (black) to observational data distribution (red): 
For the CMR component.
```{r ppd_summary_3, echo=TRUE, fig.align='center', fig.height=4, fig.width=6, message=FALSE, warning=FALSE}

la_fit_1 <- rstan::extract( igm_fit_1 ) 

bayesplot::ppc_stat( y = df_cmr$length_r ,
                     yrep = la_fit_1$y_rep_cmr , 
                     stat = mean ) + ggtitle("CMR")

bayesplot::ppc_stat( y = df_cmr$length_r , 
                     yrep = la_fit_1$y_rep_cmr , 
                     stat = sd ) + ggtitle("CMR")

bayesplot::ppc_stat( y = df_cmr$length_r , 
                     yrep = la_fit_1$y_rep_cmr , 
                     stat = max ) + ggtitle("CMR")

bayesplot::ppc_stat( y = df_cmr$length_r , 
                     yrep = la_fit_1$y_rep_cmr , 
                     stat = min ) + ggtitle("CMR")

s <- as.vector(sample(1:nrow(df_cmr), 100, replace = F ))#draw random 100 rows

#bayesplot::ppc_dens_overlay( y = df_mixture$length , yrep = la_prior$y_rep_mix[ s , ] )

bayesplot::ppc_intervals( y = df_cmr$length_r[ s ], yrep = la_fit_1$y_rep_cmr[  , s ] ) +
  ggtitle("CMR")

rm(s)

plot( density( la_fit_1$y_rep_cmr[ 1 , ] ) , lwd=2 , main= "CMR" , ylim=c( 0, 0.02 ), xlim=c( 0 , 500 ) , col='gray' )
for ( i in 1:200 ){
  lines( density( la_fit_1$y_rep_cmr[ sample( 1:dim(la_fit_1$y_rep_cmr)[1], 1 ) , ] ) , lwd=2 , col='gray' )
  }
lines( density ( df_cmr$length_r ) , col='red' , lwd=2 )

```

For the mixture component.
```{r ppd_summary_4, echo=TRUE, fig.align='center', fig.height=4, fig.width=6, message=FALSE, warning=FALSE}

bayesplot::ppc_stat( y = df_mixture$length ,
                     yrep = la_fit_1$y_rep_mix , 
                     stat = mean ) + ggtitle("Mixture")

bayesplot::ppc_stat( y = df_mixture$length , 
                     yrep = la_fit_1$y_rep_mix , 
                     stat = sd ) + ggtitle("Mixture")

bayesplot::ppc_stat( y = df_mixture$length , 
                     yrep = la_fit_1$y_rep_mix , 
                     stat = max ) + ggtitle("Mixture")

bayesplot::ppc_stat( y = df_mixture$length , 
                     yrep = la_fit_1$y_rep_mix , 
                     stat = min ) + ggtitle("Mixture")

s <- as.vector(sample(1:nrow(df_mixture), 100, replace = F ))#draw random 100 rows

#bayesplot::ppc_dens_overlay( y = df_mixture$length , yrep = la_prior$y_rep_mix[ s , ] )

bayesplot::ppc_intervals( y = df_mixture$length[ s ], yrep = la_fit_1$y_rep_mix[  , s ] ) +
  ggtitle("Mixture")

rm(s)

plot( density( la_fit_1$y_rep_mix[ 1 , ] ) , lwd=2 , main= "Mixture" , ylim=c( 0, 0.02 ), xlim=c( 0 , 500 ) , col='gray' )
for ( i in 1:200 ){
  lines( density( la_fit_1$y_rep_mix[ sample( 1:dim(la_fit_1$y_rep_mix)[1], 1 ) , ] ) , lwd=2 , col='gray' )
  }
lines( density ( df_mixture$length ) , col='red' , lwd=2 )
```

### Leave-one-out cross validation with PSIS-LOO
```{r loo_igm, fig.align='center', fig.height=4, fig.width=6 }

log_lik_igm_1 <- loo::extract_log_lik( igm_fit_1 , 
                                  parameter_name="log_lik", 
                                  merge_chains = FALSE )

r_eff_igm_1 <- loo::relative_eff( log_lik_igm_1 , cores = 1 )

loo_igm_1 <- loo::loo( log_lik_igm_1, 
                  r_eff= r_eff_igm_1 , 
                  cores = 1 )
print( loo_igm_1 )

plot( loo_igm_1 , label_points = T )

```




